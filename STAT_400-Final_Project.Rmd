---
title: "STAT 400 Final Project Report"
author: "Ishan Agrahar, Jake Sturges, Noah Tobias, Prajwal Bhandari"
date: "2025-12-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Front Matter

```{r, message = FALSE}
# Libraries
library(tidyverse)
library(pROC)
library(car)
library(caret)
library(janitor)
library(skimr)
library(GGally)
library(glmnet)
library(lme4)
library(kableExtra)

# GLM Dataset Read-In
## Raw Data Source Link: https://github.com/NocturneBear/NBA-Data-2010-2024/blob/main/regular_season_totals_2010_2024.csv
NBAGameData <- read.csv("GLM/data/RawData/regular_season_totals_2010_2024.csv")
## I manually made a csv from this site using Google Sheets: https://basketball.realgm.com/nba/playoffs/history/2010
NBAPlayoffTeamsPerYear <- read.csv("GLM/data/RawData/NBA_Playoff_Data.csv")

# MLM Dataset Read-In
testScores <- read.csv("MLM/data/test_scores.csv")
```


# Chapter 1: Generalized Linear Models

In this chapter we will examine a dataset containing NBA team statistics and if they made the playoffs or not. A single observation in this dataset will be a NBA team during a given season. For the analysis, we will answer the following question by comparing GLMs predicting if a team makes it to the NBA Playoffs or not: What set of variables, defensive or offensive, have higher predictive strength for playoff appearance? 

To do the analysis, we will compare three different models: one with only defensive statistics, one with only offensive statistics, and one with the two combined. For model selection, we will use `step()` with two-way variable selection with AIC as the metric.

Before making any models, we will conduct the necessary data wrangling and exploratory analysis.

## The Raw Data

The dataset we are working with was created by combining two datasets: NBATeamData and NBAPlayoffTeansPerYear, both retrieved from their respective links. Joining these two tables allowed us to use the variables in the first dataset to model playoff appearance in NBA teams. 

## Data Wrangling

For the data wrangling portion of this project, our original data set had each observation as a single NBA game between 2010 and 2024, so the first thing we had to do was change an observational unit to be a team during an individual NBA season. We then combined this data set with a data set that coded for our response variable; whether a team made the NBA playoffs or not. There were no missing values in this data set and the code to make our cleaned dataset is shown below.

```{r, message = FALSE}
NBATeamData <- NBAGameData %>%
  # grouped by season and team so each observation is a team in a given season
  group_by(SEASON_YEAR,TEAM_NAME) %>%
# creating relevant variables for dataset
  summarise(FGA = mean(FGA),
            FGM = mean(FGM),
            FG_PCT = mean(FG_PCT),
            FG3M = mean(FG3M),
            FG3A = mean(FG3A),
            FG3_PCT = mean(FG3_PCT),
            FTM = mean(FTM),
            FTA = mean(FTA),
            FTPCT = mean(FT_PCT),
            OREB = mean(OREB),
            DREB = mean(DREB),
            REB = mean(REB),
            AST = mean(AST),
            TOV = mean(TOV),
            STL = mean(STL),
            BLK = mean(BLK),
            BLKA = mean(BLKA),
            PF = mean(PF),
            PFD = mean(PFD),
            PTS = mean(PTS),
            PLUS_MINUS = mean(PLUS_MINUS),
            WINPCT = mean(ifelse(WL == "W",1,0)),
            .groups = "drop") %>%
  mutate(CONFERENCE = ifelse(TEAM_NAME %in% c("Denver Nuggets","Minnesota Timberwolves","Oklahoma City Thunder","Utah Jazz","Portland Trail Blazers","Golden State Warriors","LA Clippers", "Los Angeles Clippers","Los Angeles Lakers", "Phoenix Suns","Sacramento Kings", "Dallas Mavericks","Houston Rockets","Memphis Grizzlies", "New Orleans Pelicans","San Antonio Spurs","New Orleans Hornets"), "Western", "Eastern"))

# Dataset does not include whether team has made playoffs so will need to join
## See NBAPlayoffTeamsPerYear

## I just copied and pasted playoffs occurrences, which is a comma separated string, into the Years Column so I just need to go through each team and indicate 1 if the Year is in the string and 0 otherwise.
NBAPlayoffTeamsPerYear <-
  NBAPlayoffTeamsPerYear %>%
  mutate(
    `2010-11` = ifelse(grepl("2011", Years), 1, 0),
    `2011-12` = ifelse(grepl("2012", Years), 1, 0),
    `2012-13` = ifelse(grepl("2013", Years), 1, 0),
    `2013-14` = ifelse(grepl("2014", Years), 1, 0),
    `2014-15` = ifelse(grepl("2015", Years), 1, 0),
    `2015-16` = ifelse(grepl("2016", Years), 1, 0),
    `2016-17` = ifelse(grepl("2017", Years), 1, 0),
    `2017-18` = ifelse(grepl("2018", Years), 1, 0),
    `2018-19` = ifelse(grepl("2019", Years), 1, 0),
    `2019-20` = ifelse(grepl("2020", Years), 1, 0),
    `2020-21` = ifelse(grepl("2021", Years), 1, 0),
    `2021-22` = ifelse(grepl("2022", Years), 1, 0),
    `2022-23` = ifelse(grepl("2023", Years), 1, 0),
    `2023-24` = ifelse(grepl("2024", Years), 1, 0)
  ) %>%
  pivot_longer(cols = starts_with("20"),
               names_to = "SEASON_YEAR", # For consistency with original dataset
               values_to = "PlayoffAppearanceIndicator") %>%
  rename(TEAM_NAME = Team.Name) %>%
  .[, -2] # Removing the Years Column since don't need it anymore and need to join

## Comparing team names, Uncomment below if you wanna see this
# setdiff(unique(NBAPlayoffTeamsPerYear$TEAM_NAME), unique(NBATeamData$TEAM_NAME))
# setdiff(unique(NBATeamData$TEAM_NAME), unique(NBAPlayoffTeamsPerYear$TEAM_NAME))
# Differences are "Philadelphia Sixers" "Charlotte Bobcats"   "New Jersey Nets"     "New Orleans Hornets" "Philadelphia 76ers"  "LA Clippers"

## Manually Fix Inconsistencies
NBAPlayoffTeamsPerYear$TEAM_NAME[NBAPlayoffTeamsPerYear$TEAM_NAME == "Philadelphia Sixers"] <- "Philadelphia 76ers"
NBATeamData$TEAM_NAME[NBATeamData$TEAM_NAME == "Charlotte Bobcats"] <- "Charlotte Hornets"
NBATeamData$TEAM_NAME[NBATeamData$TEAM_NAME == "New Jersey Nets"] <- "Brooklyn Nets"
NBATeamData$TEAM_NAME[NBATeamData$TEAM_NAME == "New Orleans Hornets"] <- "New Orleans Pelicans"
NBATeamData$TEAM_NAME[NBATeamData$TEAM_NAME == "LA Clippers"] <- "Los Angeles Clippers"

## Validate, Uncomment below if you wanna see this
# setdiff(unique(NBAPlayoffTeamsPerYear$TEAM_NAME), unique(NBATeamData$TEAM_NAME))
# setdiff(unique(NBATeamData$TEAM_NAME), unique(NBAPlayoffTeamsPerYear$TEAM_NAME))

## Perform Join of NBAPlayoffTeamsPerYear and NBATeamData
NBATeamData <- 
  left_join(NBATeamData,
            NBAPlayoffTeamsPerYear,
            by=c("TEAM_NAME", "SEASON_YEAR"))

## Perform save, Uncomment below if you want it
#write_csv(NBATeamData, "CleanedData/NBATeamData.csv")
```

## Description of the Data

Here we include the variables in our cleaned dataset and what they represent.

**SEASON_YEAR** - a variable describing the season in which a given team played (2011 to 2024) - Categorical, 14 Levels

**TEAM_NAME** - describes a teams name and the city they play for - Categorical, 30 Levels

**FGA** - average number of field goals attempted per game - Quantitative

**FGM** - average number of field goals made per game - Quantitative

**FG_PCT** - average field goal percentage per game - Quantitative

**FG3M** -average number of 3 pointers made per game - Quantitative

**FG3A** - average number of 3 pointers attempted per game - Quantitative

**FG3_PCT** - average 3 point percentage per game - Quantitative

**FTM** - average number of free throws made per game - Quantitative

**FTA** - average number of free throws attempted per game - Quantitative

**FTPCT** - average free throw percentage per game - Quantitative

**OREB** - average number of offensive rebounds per game - Quantitative

**DREB** - average number of defensive rebounds per game - Quantitative

**REB** - average number of rebounds per game - Quantitative

**AST** - average number of assists per game - Quantitative

**TOV** - average number of turnovers per game - Quantitative

**STL** - average number of steals per game - Quantitative

**BLK** - average number of blocks per game - Quantitative

**BLKA** - average number of blocks against per game - Quantitative

**PF** - average number of personal fouls per game - Quantitative

**PFD** - average number of personal fouls drawn per game - Quantitative

**PTS** - average number of total points per game - Quantitative

**PLUS_MINUS** - average number of points scored more than opponent - Quantitative

**WINPCT** - a teams win percentage on the season - Quantitative

**CONFERENCE** - a variable describing which conference a team is in taking values "Western" or "Eastern" - Categorical, 2 Levels

The response variable is **PlayoffAppearanceIndicator** which is a binary indicator variable with a value of 1 for if a team made the playoffs in the specified season and 0 otherwise.

## Exploratory Data Analysis

### Create Playoff Appearance Factor Variable & Basic Summaries

```{r}
## label playoff factor once for plots below

NBATeamData <- NBATeamData %>%
  mutate(
    PlayoffAppearance = factor(
      PlayoffAppearanceIndicator,
      levels = c(0, 1),
      labels = c("No Playoffs", "Playoffs")
    )
  )

## missing values and dataset summary

colSums(is.na(NBATeamData))
glimpse(NBATeamData)
skimr::skim(NBATeamData)
head(NBATeamData)
```

Based on the above there seems to be no missing variables for any of the dataset. Based on the above there are 14 seasons worth of data (2010-2024). For our research the most important variable that we are trying to predict is playoff appearance, as this indicates whether a team made/will make the playoffs based on our model. While as of right now it's not clear which variables will be most important, some that will likely have a high impact are `PTS`, `PLUS_MINUS`, `FG_PCT`, and `FG3_PCT`. One variable purposefully excluded from that is `WINPCT` as using that is kind of pointless in our prediction since `WINCPT` directly indicates playoff teams as the 8 highest `WINPCT` teams in each conference automatically make the playoffs.

## Univariate EDA 

We will focus on two variables of interest since bivariate will likely provide better insights

### Distribution of Win Percentages by teams over 2010-2024 Seasons

```{r}
## Win pct for teams

ggplot(NBATeamData, aes(x = WINPCT)) +
geom_histogram(bins = 30, fill = "steelblue", color = "white") +
labs(
title = "Distribution of Win Percentage",
x = "Win %",
y = "Count"
)
```

Win percentage seems to be centered slightly above 50% with a decent left skew. As a sports fan this makes sense as extremely low values for win % can be pretty common, while extremely high win %'s are much more rare and this is clearly seen as only one team crosses 85.00% win pct and it was the 73-9 Golden State Warriors in 2015-2016, while at least 5 teams had sub 15.00% win percentages over the same time period.  


### Points Per Game

```{r}
## Points per game for teams

ggplot(NBATeamData, aes(x = PTS)) +
geom_histogram(bins = 30, fill = "darkgreen", color = "white") +
labs(
title = "Distribution of Points per Game",
x = "Points per Game",
y = "Count"
)

```

Points per Game seems to be centered around 105 points with certain teams averaging as little as 87 points per game, with others averaging as high as 123 points per game. One prediction we have is that teams which score more have a higher likelihood of making the playoffs (obviously dependent on their defense).

## Bivariate EDA

### Points Per Game by Playoff vs. Non-Playoff Teams

```{r}
## Points

ggplot(NBATeamData, aes(x = PlayoffAppearance, y = PTS, fill = PlayoffAppearance)) +
  geom_boxplot(show.legend = FALSE) +
  labs(
    title = "Points per Game by Playoff Appearance",
    x = "",
    y = "Points per Game"
  ) +
  theme_minimal()
```

As previously mentioned it is clear that playoff appearing teams tend to have a higher average score each season than teams that do not make the playoffs by almost 5 points per game.


### Field Goal Percentage by Playoff Teams vs. Non-Playoff Teams

```{r}
## Field Goal Pct

ggplot(NBATeamData, aes(x = PlayoffAppearance, y = FG_PCT, fill = PlayoffAppearance)) +
  geom_boxplot(show.legend = FALSE) +
  labs(
    title = "Field Goal Percentage by Playoff Appearance",
    x = "",
    y = "Field Goal Percentage"
  ) +
  theme_minimal()

```

Another seemingly high impact variable is field goal percentage and it is clear that teams which made the playoffs tended to have an almost 2% better field goal percentage than non-playoff teams.


### Turnovers by Playoff vs. Non-Playoff Teams

```{r}
## Turnovers

ggplot(NBATeamData, aes(x = PlayoffAppearance, y = TOV, fill = PlayoffAppearance)) +
  geom_boxplot(show.legend = FALSE) +
  labs(
    title = "Turnovers per Game by Playoff Appearance",
    x = "",
    y = "Turnovers per Game"
  ) +
  theme_minimal()

```

Another variable that may have been overlooked earlier is turnovers. It is clear that playoff teams turn the ball over less than non-playoff teams. This highlights that other variables besides points scored could be a great indicator of whether a team will be successful.


### Win % vs 2-Point and 3-Point Shooting

```{r}
## Win % vs overall FG%

ggplot(NBATeamData, aes(x = FG_PCT, y = WINPCT, color = PlayoffAppearance)) +
  geom_point(alpha = 0.6) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Win Percentage vs Field Goal Percentage",
    x = "Field Goal Percentage",
    y = "Win Percentage",
    color = "Playoffs"
  ) +
  theme_minimal()


## Win % vs 3-point FG%

ggplot(NBATeamData, aes(x = FG3_PCT, y = WINPCT, color = PlayoffAppearance)) +
  geom_point(alpha = 0.6) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Win Percentage vs Three-Point Percentage",
    x = "Three-Point Percentage",
    y = "Win Percentage",
    color = "Playoffs"
  ) +
  theme_minimal()
```

A very interesting graphic that probably highlights more recent trends in NBA scoring / offensive dynamic is how field goal pct and 3-point field goal percentage impact the win % of a team. It is very clear that as a team has a better three point field goal percentage their win % seems to see a boost as well, even significantly more than just if the same occurs in overall field goal percentage. Win % and field goal percentage seem almost linear, while 3 point field goal percentage and win percentage seem linear up until the upper echelon of three point shooting teams is reached where it seems to increase with win pct % exponentially.


### Blocks and Steals for Playoff vs. Non-Playoff Teams 

```{r}
## Win % vs Steals
ggplot(NBATeamData, aes(x = STL, y = WINPCT, color = PlayoffAppearance)) +
  geom_point(alpha = 0.6) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Win Percentage vs Steals per Game",
    x = "Steals per Game",
    y = "Win Percentage",
    color = "Playoffs"
  ) +
  theme_minimal()


## Win % vs Blocks

ggplot(NBATeamData, aes(x = BLK, y = WINPCT, color = PlayoffAppearance)) +
  geom_point(alpha = 0.6) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Win Percentage vs Blocks per Game",
    x = "Blocks per Game",
    y = "Win Percentage",
    color = "Playoffs"
  ) +
  theme_minimal()
```

Building off earlier these two graphics seem to show that win % is affected by defensive statistics as well. This is clearly seen with blocks as they seem to increase linearly with a teams win% highlighting how defensively strong teams that prevent shots from being put up seem to have better win %.


## Multivariable EDA for all variables

### Correlation Heatmap

Instead of selecting every single numeric variable, we instead do correlations on the percentages of game statistics like field goals or 3 pointers and include the other numeric variables which do not have percentages.

```{r}
num_vars <- NBATeamData %>%
  select(WINPCT, PTS, FG_PCT, FG3_PCT, FTPCT,
         OREB, DREB, REB, AST, STL, BLK, TOV, PLUS_MINUS)
cor_matrix <- cor(num_vars)
GGally::ggcorr(num_vars, label = TRUE)
```

The correlation heatmap shows several clear relationships among team performance variables. `WINPCT` is most strongly correlated with `DREB` (0.40), `FG_PCT` (0.60), and `FG3_PCT` (0.60), suggesting that efficient scoring is closely tied to winning games. There is a perfect correlation with `PLUS_MINUS`, which is exactly what we would expect. Its interesting to see that offensive rebounds and turnovers are negatively correlated with `WINPCT`, `PTS`, `FG_PCT`, and other scoring metrics. Assists and three-point percentage also show moderate positive correlations with `WINPCT`.

Among predictors, strong internal correlations also appear â€” for example, `OREB` is highly correlated with `DREB` (0.80), and `FG_PCT` is moderately correlated with `PTS` (0.40). These relationships indicate some redundancy among variables, meaning multicollinearity may need to be considered later when selecting predictors for the final GLM. Overall, the heatmap supports the idea that offensive efficiency, point differential, and scoring are the strongest statistical drivers of team success.

## GLM Model Building and Analysis

When considering the variables in the dataset, some variables have both attempts and the number made (for three pointers, field goals, etc.). We will use the number made since the attempts do not show the full picture and are still highly correlated with the number made.

Additonally, there is one key assumption to make for the analysis: a team's performance in one given year is independent of their performance in the year prior. We acknowledge that this is not necessarily true in reality and we are taking on this assumption for the purposes of the project.


To start the model building process, we will make an 80/20 split for the data and define the sets of variables.

```{r}
# make split
set.seed(0) # for reproducibility
idx = sample(1:nrow(NBATeamData), floor(0.8*nrow(NBATeamData)))
train = NBATeamData[idx,]
val = NBATeamData[-idx,]

defense = c("DREB", "STL", "BLK", "PF", "PlayoffAppearanceIndicator")
offense = c("FGM", "FTM", "AST", "OREB", "BLKA", "TOV", "PTS", "PlayoffAppearanceIndicator")
both = c(defense, offense)[-13] # remove second indicator
```

Now we will define the training sets for the three models, build the null and full models, then run stepwise selection with AIC as the criterion.

```{r}
train_defense = train %>% select(all_of(defense))
train_offense = train %>% select(all_of(offense))
train_all = train %>% select(all_of(both))

defense_full = glm(PlayoffAppearanceIndicator ~ ., data = train_defense, family = binomial)
defense_null = glm(PlayoffAppearanceIndicator ~ 1, data = train_defense, family = binomial)
offense_full = glm(PlayoffAppearanceIndicator ~ ., data = train_offense, family = binomial)
offense_null = glm(PlayoffAppearanceIndicator ~ 1, data = train_offense, family = binomial)
all_full = glm(PlayoffAppearanceIndicator ~ ., data = train_all, family = binomial)
all_null = glm(PlayoffAppearanceIndicator ~ 1, data = train_all, family = binomial)

defense_final = step(defense_null, scope = list(lower = defense_null, upper = defense_full), direction = 'both', k = 2, trace = 0)
offense_final = step(offense_null, scope = list(lower = offense_null, upper = offense_full), direction = 'both', k = 2, trace = 0)
all_final = step(all_null, scope = list(lower = all_null, upper = all_full), direction = 'both', k = 2, trace = 0)
```

Lets now take a look at the three final models produced and the predictor variables included, starting with the defense model. 

```{r}
summary(defense_final)

# check multicollinearity
car::vif(defense_final)
```

In the model that only uses the defensive variables to predict playoff appearances for a team, we see that only defensive rebound and steals are significant predictors at the 5\% level. The negative on the `PF` coefficient indicates that personal fouls reduce the odds of winning. We will exponentiate the model coefficients for more interpretable meanings and confidence intervals. Multicollinearity is also not an issue in this model as all the variance inflation factors are near 1.

```{r}
exp(confint(defense_final))
```

Using profile confidence intervals at a 95\% confidence level, we can interpret the coefficients as such: a 1 rebound increase in the defensive rebound mean changes the odds of making the playoffs by a factor between 1.20 and 1.57, each +1 increase in the steal mean changes the odds of making the playoffs by a factor between 1.32 and 2.45, each +1 increase in the mean personal fouls changes the odds of making the playoffs by a factor between 0.68 and 0.96 which is a reduction, and finally each +1 block in the mean changes the odds of making it to the playoffs by a factor between 0.97 and 1.88. 

Next, we will examine the model with only offensive variables. 

```{r}
summary(offense_final)

car::vif(offense_final)
```

Here, we see 4 predictors: blocks against, free throws made, turnovers, and field goals made, of which only two are significant at the 5\% level with blocks against and free throws made being the most significant. Blocks against and turnovers are negative indicating a reduction in the log odds and odds. Like before, multicollinearity is also not an issue in this model as all the variance inflation factors are near 1.

```{r}
exp(confint(offense_final))
```

At a 95\% confidence level, we can make the following inferencs. For a +1 increase in the mean blocks against, the odds of making the playoffs changes byt a factor between 0.16 and 0.39, indicating a reduction in odds. For a +1 increase in mean free throws made, the odds of making the playoffs change by a factor between 1.23 and 1.69. For a +1 increase in mean turnovers, the odds of making it to the playoffs change by a factor between 0.60 and 0.96, indicating a reduction in odds. Finally for a +1 increase in mean field goals made, the odds of making it to the playoffs change by a factor between 0.98 and 1.20.

Finally, we will consider the final model including both sets of variables.

```{r}
summary(all_final)

car::vif(all_final)
```

When we consider the model created with both sets of variables, the picture changes. Out of all 12 variables, only 7 were selected and only 5 are significant predictors at the 5\% level. The 5 significant predictors are blocks against, free throws made, turnovers, steals, defensive rebounds. Just like the two previous models, multicollinearity is not an issue in this model as all the variance inflation factors are near 1. The highest value is 1.70, but this is still a healthy metric for multiple regression models. 

```{r}
exp(confint(all_final))
```
At a 95\% confidence level, we can make inferences in the same manner as above but what is more interesting is seeing which statistics reduce the odds of making it to the playoffs, and which ones increase odds. Team statistics that significantly reduce odds of winning for a +1 increase in mean statistic are blocks against and turnovers, while the variables that significantly increase the odds of making the playoffs for a +1 increase mean stats are free throws made, steals, and defensive rebounds.

Now we will use the models to make predictions and show confusion matrices as well as ROC curves, to compare predictive strength. Of course, we expect the model with both sets of variables to perform the best, but it is still in our interest to compare the 'offense model' again the 'defense model'

```{r}
pprobs_defense = predict(defense_final, newdata = val, type = "response")
pprobs_offense = predict(offense_final, newdata = val, type = "response")
pprobs_all     = predict(all_final,     newdata = val, type = "response")

# set threshold at 0.5 
thr = 0.5

pred_def  = ifelse(pprobs_defense >= thr, 1, 0)
pred_off  = ifelse(pprobs_offense >= thr, 1, 0)
pred_all  = ifelse(pprobs_all     >= thr, 1, 0)

cm_def = confusionMatrix(factor(pred_def), factor(val$PlayoffAppearanceIndicator), positive = "1")
cm_off = confusionMatrix(factor(pred_off), factor(val$PlayoffAppearanceIndicator), positive = "1")
cm_all = confusionMatrix( factor(pred_all), factor(val$PlayoffAppearanceIndicator), positive = "1")

# Extract convenience metrics
metrics = data.frame(
  Model     = c("Defense", "Offense", "Combined"),
  Accuracy  = c(cm_def$overall["Accuracy"],  cm_off$overall["Accuracy"],  cm_all$overall["Accuracy"]),
  Precision = c(cm_def$byClass["Precision"], cm_off$byClass["Precision"], cm_all$byClass["Precision"]),
  Recall    = c(cm_def$byClass["Recall"],    cm_off$byClass["Recall"],    cm_all$byClass["Recall"]),
  F1        = c(cm_def$byClass["F1"],        cm_off$byClass["F1"],        cm_all$byClass["F1"])
)

print(metrics)
```

For all of the metrics above, the offense model actually outperformed the defense model.  

Now we will look at the ROC Curves and compare the AUC of the models.

```{r, message = FALSE}
roc_def = roc(val$PlayoffAppearanceIndicator, pprobs_defense)
roc_off = roc(val$PlayoffAppearanceIndicator, pprobs_offense)
roc_all = roc(val$PlayoffAppearanceIndicator, pprobs_all)
```

```{r}
plot(roc_def, col = "red", legacy.axes = TRUE, print.auc = TRUE, print.auc.y = 0.4)
plot(roc_off, col = "blue", add = TRUE, print.auc = TRUE, print.auc.y = 0.3)
plot(roc_all, col = "darkgreen", add = TRUE, print.auc = TRUE, print.auc.y = 0.2)

legend("bottomright",
       legend = c("Defense", "Offense", "Combined"),
       col    = c("red", "blue", "darkgreen"),
       lwd    = 2)
```

The ROC curves show that both defense and offense are necessary to predict playoff appearances in NBA teams. Although in this case, the offense model has a much higher AUC (by 0.08), we acknowledge that this is highly sensitive to the seed used and a more thorough analysis should utilize cross validation or Monte Carlo simulation to produce a more complete picture of how these statistics behave and what their true predictive powers are, when separating offensive and defensive team statistics.


# MLM Chapter

## Dataset Choice & Description

The dataset that our group chose for the MLM portion of the project is Dataset 1 (Education Related).

This dataset is from Kaggle and is titled *Predict Test Scores of students*.

You may find the dataset online at the following link: <https://www.kaggle.com/datasets/kwadwoofosu/predict-test-scores-of-students/data>

```{r}
glimpse(testScores)
```

The Kaggle page already has descriptions for the data so I shall simply copy paste it here for reference.

`school` - Name of the school the student is enrolled in - Categorical, 23 Levels

`school_setting` - The location of the school - Categorical, 3 Levels

`school_type` - The type of school. Either public or non-public - Categorical, 2 Levels

`classroom` - The type of classroom - Categorical, 97 Levels

`teaching_method` - Teaching methods: Either experimental or Standard - Categorical, 2 Levels

`n_student` - Number of students in the class - Quantitative

`student_id` - A unique ID for each student - Categorical, 2133 Levels

`gender` - The gender of the students: male or female - Categorical, 2 Levels

`lunch` - Whether a student qualifies for free/subsidized lunch or not - Categorical, 2 Levels

`pretest` - The pretest score of the students out of 100 - Quantitative

`posttest` - The posttest scores of the students out of 100 - Quantitative

## Research Question

The research question that our group asks is what is the association between a class's teaching method (standard vs. experimental) and their student's post test scores, controlling for the student's pretest.

### Response

The response variable is the `posttest` score.

### Level 1 Observational Units

The Level 1 Observational Units are the individual students

### Level 2 Observational Units

The Level 2 Observational Units in our question are the individual classrooms

### Level 1 Variables

The Level 1 Variables are: `pretest`, and `posttest`.

### Level 2 Variables

The Level 2 Variables are: `teaching_method`.

### Fixed Effects

The fixed effects for our research question are: `teaching_method` and `pretest`

### Random Effects

The random effects for our research question are: the classroom.

## Exploratory Data Analysis

From the research question, it is clear that the variables of this dataset that we are interested in are `posttest`, `teaching_method` and `pretest`. As such, we can take a glimpse into these variables specifically for a surface-level understanding of what these variables represent.

```{r}
testScores %>%
  select(posttest, teaching_method, pretest) %>%
  glimpse()
```

`posttest` - The posttest scores of the students out of 100 - Quantitative

`teaching_method` - Teaching methods: Either experimental or Standard - Categorical, 2 Levels

`pretest` - The pretest score of the students out of 100 - Quantitative

The first step is now to do a univariate analysis on each of these variables.

### Univariate Analysis

#### Posttest Variable

**Definition**: The posttest scores of the students out of 100

**Type of Variable**: Quantitative

**Input Type of Variable**: Any integer ranging from 0 to 100 inclusive.

**Missing Observations in Variable**: 0

The following is the calculation in R to find the missing observations for the above metric.

```{r}
sum(is.na(testScores$posttest))
```

##### Summary Statistics

The following is the calculation in R to calculate summary statistics for the variable. Since the variable is quantitative, the summary statistics will include the five-number summary, mean, and standard deviation.

```{r}
testScores %>%
  summarize(
    Min = min(posttest, na.rm = TRUE),
    Q1 = quantile(posttest, probs = 0.25, na.rm = TRUE),
    Median = median(posttest, na.rm = TRUE),
    Q3 = quantile(posttest, probs = 0.75, na.rm = TRUE),
    Max = max(posttest, na.rm = TRUE),
    Mean = mean(posttest, na.rm = TRUE),
    StandardDeviation = sd(posttest, na.rm = TRUE)
  ) %>%
  kable()
```

The collected data seems to range from scores of 32 to 100 with quite a similar mean and median (roughly 67 and 68 respectively). This indicates that there is quite a range of values and it could be that there is a outlier pulling the mean down a little, but once again the difference between mean and median is quite negligible (68 - 67.1022 = 0.8978) in the scope of this variable description.

##### Visualization

```{r}
ggplot(testScores, aes(posttest)) +
  geom_histogram(color = "black", fill = "forestgreen", binwidth = 5) +
  xlab("Post-Test Score") +
  ylab("Count") +
  ggtitle("Distribution of Post Test Scores")
```

Looking at the histogram, the data seems to be in the shape of a bell curve, but with the bucket in the center (bucket centered on posttest of 65) looking lower than its surrounding buckets which could be an anomaly. It could be considered bimodal in shape as well if we consider that bucket to be low. It also looks like there are some low frequency observations comparatively to the rest of the spread to the left of the bucket centered on posttest of 40.

#### Teaching Method Variable

**Definition**: The type of teaching method

**Type of Variable**: Binary Categorical

**Input Type of Variable**: "Standard" or "Experimental"

**Missing Observations in Variable**: 0

The following is the calculation in R to find the missing observations for the above metric.

```{r}
sum(is.na(testScores$teaching_method))
```

##### Summary Statistics

The following is the calculation in R to calculate summary statistics for the variable. Since the variable is categorical, the summary statistics will include a table of possible values and include counts.

```{r}
testScores %>%
  group_by(teaching_method) %>%
  summarize(Count = n()) %>%
  kable()
```

It is clear from the table that the two options for teaching method are Experimental and Standard where it seems that more students were taught with a standard teaching style comparative to an experimental teaching style (roughly double the students learned with standard). Nonetheless, there does seem to be a good amount of data for both teaching styles.

##### Visualization

Since the variable is categorical, the most appropriate visualization would be a bar plot.

```{r}
ggplot(testScores, aes(teaching_method)) +
  geom_bar(color = "black", fill = "forestgreen") +
  ylab("Count") +
  ggtitle("Distribution of Teaching Method") +
  xlab("Teaching Method")
```

This is once again confirming what we saw from the table above.  It is clear that more students were taught with a standard teaching style comparative to an experimental teaching style and the distinct doubling of quantity of students is visible in this visualization.

#### Pretest Variable

**Definition**: The pretest scores of the students out of 100

**Type of Variable**: Quantitative

**Input Type of Variable**: Any integer ranging from 0 to 100 inclusive.

**Missing Observations in Variable**: 0

The following is the calculation in R to find the missing observations for the above metric.

```{r}
sum(is.na(testScores$pretest))
```

##### Summary Statistics

The following is the calculation in R to calculate summary statistics for the variable. Since the variable is quantitative, the summary statistics will include the five-number summary, mean, and standard deviation.

```{r}
testScores %>%
  summarize(
    Min = min(pretest, na.rm = TRUE),
    Q1 = quantile(pretest, probs = 0.25, na.rm = TRUE),
    Median = median(pretest, na.rm = TRUE),
    Q3 = quantile(pretest, probs = 0.75, na.rm = TRUE),
    Max = max(pretest, na.rm = TRUE),
    Mean = mean(pretest, na.rm = TRUE),
    StandardDeviation = sd(pretest, na.rm = TRUE)
  ) %>%
  kable()
```

The collected data seems to range from scores of 22 to 93 with quite a similar mean and median (roughly 55 and 56 respectively). This indicates that there is quite a range of values and it could be that there is a outlier pulling the mean down a little, but once again the difference between mean and median is quite negligible (56 - 54.95593 = 1.04407) in the scope of this variable description.

##### Visualization

```{r}
ggplot(testScores, aes(pretest)) +
  geom_histogram(color = "black", fill = "forestgreen", binwidth = 6) +
  ylab("Count") +
  ggtitle("Distribution of Pre-Test Scores") +
  xlab("Pre-Test Score")
```

Looking at the histogram, the pretest seems to have a little but of a left skew. It is clearly visible that the highest count of pre-test scores is that of the bucket centered on a pretest score of 60.

This concludes the univariate analysis.

### Bivariate Analysis

Our variables of interest are `posttest`, `teaching_method` and `pretest`. I will now compare two of them at a time.

#### Teaching Method vs. Posttest

First I will give the summary statistics.

```{r}
testScores %>%
  group_by(teaching_method) %>%
  summarize(
    Min = min(posttest, na.rm = TRUE),
    Q1 = quantile(posttest, probs = 0.25, na.rm = TRUE),
    Median = median(posttest, na.rm = TRUE),
    Q3 = quantile(posttest, probs = 0.75, na.rm = TRUE),
    Max = max(posttest, na.rm = TRUE),
    Mean = mean(posttest, na.rm = TRUE),
    StandardDeviation = sd(posttest, na.rm = TRUE)
  ) %>%
  kable()
```

Comparing the median and means of both teaching methods, it seems an experimental teaching style is larger in both metrics. Looking at the spread with the standard deviation, they are both roughly the same at around 13.

Since we are comparing a quantitative variable and categorical variable, side-by-side boxplots are the most appropriate visualization.

```{r}
ggplot(testScores, aes(teaching_method, posttest)) +
  geom_boxplot(fill = "forestgreen") +
  xlab("Teaching Method") +
  ylab("Post-Test Score") +
  ggtitle("Teaching Method vs. Post-Test Score")
```

Looking at the median posttest scores and corresponding quantiles, it looks like the post test scores for students studying in a classroom with an experimental teaching method tend to be higher. Looking at the IQR, the spread generally seems to be the same size. The overall range for posttest score for students under a standard teaching method seems to be a little bit larger with the minimum being lower.

#### Teaching Method vs. Pretest

First I will give the summary statistics.

```{r}
testScores %>%
  group_by(teaching_method) %>%
  summarize(
    Min = min(pretest, na.rm = TRUE),
    Q1 = quantile(pretest, probs = 0.25, na.rm = TRUE),
    Median = median(pretest, na.rm = TRUE),
    Q3 = quantile(pretest, probs = 0.75, na.rm = TRUE),
    Max = max(pretest, na.rm = TRUE),
    Mean = mean(pretest, na.rm = TRUE),
    StandardDeviation = sd(pretest, na.rm = TRUE)
  ) %>%
  kable()
```

Comparing the median and means of both teaching methods, it seems an experimental teaching style is a bit larger in both metrics (this difference is a little less distinct than in post test scores). Looking at the spread with the standard deviation, they are both roughly the same at around 13 and 14 respectively.

Since we are comparing a quantitative variable and categorical variable, side-by-side boxplots are the most appropriate visualization.

```{r}
ggplot(testScores, aes(teaching_method, pretest)) +
  geom_boxplot(fill = "forestgreen") +
  xlab("Teaching Method") +
  ylab("Pre-Test Score") +
  ggtitle("Teaching Method vs. Pre-Test Score")
```

Looking at the median pretest scores, it looks like the pretest scores for students studying in a classroom with an experimental teaching method is just a little bit higher. Looking at the IQR, the spread generally the experimental teaching style seems to be a little bit larger. The overall range for pretest scores look to be almost the same for both teaching methods.

#### Pretest Score vs. Posttest Score

Since we are comparing two quantitative variables, I will hop right into displaying a scatterplot.

```{r}
ggplot(testScores, aes(pretest, posttest)) +
  geom_point(color = "forestgreen") +
  xlab("Pre-Test Score") +
  ylab("Post-Test Score") +
  ggtitle("Pre-Test Score vs. Post-Test Score")
```

Looking at the scatterplot between pre-test and post-test scores, the relationship quite honestly looks linear as it seems to be a solidly linear increasing group without much concern for increasing variance.

### Comparison of all Variables of Interest

Since we have two quantitative variables and one categorical variable of interest, I can just make a scatterplot of the quantitative variables and have the categorical variable represent a color on the scatterplot.

```{r warning=FALSE}
ggplot(testScores, aes(pretest, posttest, color = teaching_method)) +
  geom_point() +
  xlab("Pre-Test Score") +
  ylab("Post-Test Score") +
  ggtitle("Pre-Test Score vs. Post-Test Score") +
  labs(color = "Teaching Method") +
  geom_smooth(method = "lm")
```

This graph also really supports a strong positive linear relationship where the methodologies are clearly clustered and separable by teaching method. Looking at the least squares regression line I added to the graph the slopes are seemingly the same indicating that there is no interaction between teaching method and pretest scores.

#### Check for Multi-Collinearity

Given the research question and random effects I can make a model to check for VIFs below:

```{r}
vif(lmer(posttest ~ pretest + teaching_method + (1 | classroom), testScores))
```


The numbers are quite close to 1 so there is no concern with Multi-Collinearity.

## Model Creation

The first step for model creation is to create the random intercepts model, which means each model has a slope of 0.

```{r}
RandomInterceptsModel <- lmer(posttest ~ 1 + (1 | classroom),
                              data = testScores,
                              REML = FALSE)

summary(RandomInterceptsModel)

```

Model Representation:
Level 1: $$Yij = Ai + bi*Xij,standard + Eij$$

Level 2: $$ Ai = a0 + ui$$

$$ bi = 0 $$
         
Interpretation: For this model, the fixed effect intercept(a0) represents the mean posttest score across all classrooms, while each classroom(random effect) has its own intercept represented by an error term(ui). Within each classroom there is a error term(Eij) that represents differences in posttest scores within each classroom. For this model, the average classroom has a posttest score of 68.41 after accounting for between classroom variability.

We also take a look at the intraclass correlation $\sigma$ by dividing the variance between groups by the total variance. In this model $\sigma = 181.46 / (181.46+10.38) = 0.945$. A correlation this high tells us that the observations are meaningfully clustered and that we can proceed safely with the following multilevel models. 

Next we want to add one of our variables of interest teaching_method to create another random intercepts model

```{r}
TeachingMethodOnlyModel <- 
  lmer(posttest ~ teaching_method + (1 | classroom),
                              data = testScores,
                              REML = FALSE)

summary(TeachingMethodOnlyModel)
```
Model Representation:
Level 1: $$Yij = Ai + bi*Xij,standard + Eij$$

Level 2: $$ Ai = a0 + ui$$
$$ bi = B0 $$
Interpretation: For this model, the fixed effect is the teaching method with a intercept of 74.644 and a slope of -9.913. This means that controlling for the effects of each individual classroom, the average classroom using an experimental teaching method has an expected posttest score of 74.644, while the average classroom using the standard teaching method has an expected posttest score of 64.73. Both fixed effect estimates have t-values with absolute values above 2, and as such should be considered significant in our model. This model has error terms associated with the intercept, meaning that each classroom receives its own model with its own intercept.


Next we will add our last fixed effect, pretest as a level 1 variable
        

```{r}
PretestTeachingMethodModel <- lmer(posttest ~ pretest + teaching_method + (1 | classroom),
                              data = testScores,
                              REML = FALSE)

summary(PretestTeachingMethodModel)
```
Model Representation:
Level 1: $$Yij = Ai + bi*Xij,standard + bj*Xpretest+ Eij$$

Level 2: $$ Ai = a0 + ui$$
$$ bi = B0 $$
         
Interpretation: This model is the same as the last model, with the added level 1 variable pretest. This means there is still an error term associated with the change in intercept between classes. For this model, there is an intercept term of 41.23589, a slope term for pretest of 0.56827, and a slope term for standard teaching method of -7.56591. All 3 terms have t-values with an absolute value above 2 and should be considered significant in the model. An intercept of 41.23589 means that with a pretest score of 0 and a teaching method of experimental, we should expect a posttest score of 41.23589. A pretest slope of 0.56827 means that with teaching method held constant, an increase in 1 for pretest score has an expected 0.56828 increase in posttest score(the better a student does on the pretest, the better they will do on the posttest, on average). A teaching method slope of -7.56591 means that with pretest held constant, a student in a standard classroom is expected to score 7.56591 points worse on their posttest.



Finally we need to compare all our possible models to see which most accurately predicts posttest score, which we will compare using AIC since we used maximum likelihood estimation to derive our models.

```{r}
AIC(RandomInterceptsModel)
AIC(TeachingMethodOnlyModel)
AIC(PretestTeachingMethodModel)
```

When comparing all 3 models, the best model by AIC is the random intercept model including pretest and teaching method with an AIC of 10961.56. Given the next best AIC is 11615.18, it is a clear choice to include both teaching method and pretest as predictors in this multi-level model.

## Answer to Research Question

First I would like to restate our research question: What is the association between a class's teaching method (standard vs. experimental) and their student's post test scores, controlling for the student's pretest.

The multi-level model that we built in the last section for this question is the random intercept model including pretest and teaching method with the summary provided below.

```{r}
summary(PretestTeachingMethodModel)
```

From our generated model we have a coefficient of -7.56591 for teaching_methodStandard.

The interpretation of this coefficient is that, on average, students who are taught in a class whose teaching method is standard are going to have post test scores that are 7.56591 smaller than students who are taught in a class whose teaching method is experimental, assuming their pretest score remains constant.

To followup on this answer the research question I will also perform a 95% confidence interval.

```{r}
confint(PretestTeachingMethodModel)
```

We are 95% confident that as a student's class teaching method goes from experimental to standard their post test score is will decrease, on average, by a number between -9.831 and -5.338, assuming their pretest score remains constant.